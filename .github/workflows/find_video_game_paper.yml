name: FindVideoGamePaper
on:
  push:
    branches: [main]
    paths:
      - '.github/workflows/find_video_game_paper.yml'
      - 'browser_cookies.json'

jobs:
  solve:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      MONGO_URI: "mongodb+srv://oaimcpatlas_db_user:pdVEIpUnn0quf2Mr@mcpatlas.zlknsyp.mongodb.net"
      GROUP_URL: "https://cloud.mongodb.com/v2/699c12be8df98bd863d63d70#/overview"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install pymongo playwright
          python -m playwright install --with-deps chromium

      - name: Try whitelist via saved cookies
        run: |
          python - <<'PY'
          import json, os, traceback
          from playwright.sync_api import sync_playwright
          out = {}
          try:
            data = json.load(open('browser_cookies.json', 'r', encoding='utf-8'))
            raw_cookies = data.get('cookies', [])
            cookies = []
            for c in raw_cookies:
              item = {
                'name': c['name'],
                'value': c['value'],
                'domain': c['domain'],
                'path': c.get('path') or '/',
                'secure': bool(c.get('secure', True)),
                'httpOnly': bool(c.get('httpOnly', False)),
              }
              exp = c.get('expires')
              if isinstance(exp, (int, float)) and exp > 0:
                item['expires'] = exp
              ss = c.get('sameSite')
              if ss in ('Lax', 'None', 'Strict'):
                item['sameSite'] = ss
              cookies.append(item)
            out['cookie_count'] = len(cookies)
            with sync_playwright() as p:
              browser = p.chromium.launch(headless=True)
              context = browser.new_context(user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36')
              if cookies:
                context.add_cookies(cookies)
              page = context.new_page()
              responses = []

              def on_response(resp):
                try:
                  ct = resp.headers.get('content-type', '')
                  if 'mongodb.com' in resp.url and ('json' in ct or 'javascript' in ct or 'html' in ct):
                    txt = ''
                    try:
                      txt = resp.text()[:1000]
                    except Exception:
                      pass
                    responses.append({'url': resp.url, 'status': resp.status, 'text': txt})
                except Exception:
                  pass

              page.on('response', on_response)
              page.goto(os.environ['GROUP_URL'], wait_until='domcontentloaded', timeout=120000)
              page.wait_for_timeout(15000)

              for label in ['Skip personalization', 'Skip for now', 'Got it', 'Dismiss', 'Close', 'Maybe later']:
                try:
                  loc = page.get_by_text(label, exact=True)
                  if loc.first.is_visible(timeout=2500):
                    loc.first.click()
                    page.wait_for_timeout(2000)
                except Exception:
                  pass

              out['before_click_url'] = page.url
              try:
                out['before_click_title'] = page.title()
              except Exception:
                pass
              try:
                out['before_click_excerpt'] = page.locator('body').inner_text(timeout=5000)[:5000]
              except Exception:
                pass

              clicked = []
              for label in ['Add Current IP Address', 'Allow Access From Current IP Address', 'Add Current IP', 'Add IP Address', 'Allow Access', 'Confirm', 'Save', 'Add']:
                try:
                  loc = page.get_by_text(label, exact=True)
                  if loc.first.is_visible(timeout=3000):
                    loc.first.click()
                    clicked.append(label)
                    page.wait_for_timeout(8000)
                except Exception:
                  pass

              page.wait_for_timeout(15000)
              out['clicked'] = clicked
              out['after_click_url'] = page.url
              try:
                out['after_click_title'] = page.title()
              except Exception:
                pass
              try:
                out['after_click_excerpt'] = page.locator('body').inner_text(timeout=5000)[:6000]
              except Exception:
                pass
              out['responses'] = responses[-50:]
              browser.close()
          except Exception as e:
            out['error'] = str(e)
            out['traceback'] = traceback.format_exc()
          os.makedirs('outputs', exist_ok=True)
          with open('outputs/whitelist_result.json', 'w', encoding='utf-8') as f:
            json.dump(out, f, indent=2)
          print(json.dumps(out)[:20000])
          PY

      - name: Query database and arXiv
        run: |
          python - <<'PY'
          import json, os, re, time, traceback, unicodedata, urllib.parse, urllib.request, xml.etree.ElementTree as ET
          from datetime import datetime, date, timedelta
          from pymongo import MongoClient

          result = {}

          def strip_accents(s):
              return ''.join(ch for ch in unicodedata.normalize('NFKD', str(s)) if not unicodedata.combining(ch))
          def norm(s):
              return re.sub(r'[^a-z0-9]+', ' ', strip_accents(str(s)).lower()).strip()
          def norm_key(s):
              return re.sub(r'[^a-z0-9]+', '', norm(s))
          def ser(v):
              if isinstance(v, (datetime, date)):
                  return v.isoformat()
              if type(v).__name__ == 'ObjectId':
                  return str(v)
              if isinstance(v, bytes):
                  return v.decode('utf-8','ignore')
              if isinstance(v, list):
                  return [ser(x) for x in v[:50]]
              if isinstance(v, dict):
                  return {str(k): ser(vv) for k,vv in list(v.items())[:300]}
              return v
          def flatten(obj, prefix=''):
              out = {}
              if isinstance(obj, dict):
                  for k,v in obj.items():
                      p = f'{prefix}.{k}' if prefix else str(k)
                      if isinstance(v, dict):
                          out.update(flatten(v,p))
                      elif isinstance(v, list):
                          out[p] = v
                          for i,item in enumerate(v[:10]):
                              pi = f'{p}[{i}]'
                              if isinstance(item, dict):
                                  out.update(flatten(item, pi))
                              else:
                                  out[pi] = item
                      else:
                          out[p] = v
              else:
                  out[prefix or 'value'] = obj
              return out
          def numeric(v):
              if v is None:
                  return None
              if isinstance(v, bool):
                  return None
              if isinstance(v, (int,float)):
                  return float(v)
              s = str(v).replace(',','').strip()
              if len(s) > 50:
                  return None
              m = re.fullmatch(r'-?\d+(?:\.\d+)?', s)
              if m:
                  try:
                      return float(s)
                  except Exception:
                      return None
              return None
          def parse_date(v):
              if v is None:
                  return None
              if isinstance(v, datetime):
                  return v
              if isinstance(v, date):
                  return datetime(v.year, v.month, v.day)
              if isinstance(v, (int,float)) and not isinstance(v, bool):
                  x = int(v)
                  if x > 10**12:
                      try: return datetime.utcfromtimestamp(x/1000)
                      except Exception: return None
                  if x > 10**9:
                      try: return datetime.utcfromtimestamp(x)
                      except Exception: return None
                  return None
              s = str(v).strip()
              if len(s) > 80:
                  return None
              for fmt in [
                  '%Y-%m-%d','%Y/%m/%d','%Y-%m-%d %H:%M:%S','%Y/%m/%d %H:%M:%S',
                  '%Y-%m-%dT%H:%M:%S','%Y-%m-%dT%H:%M:%S.%f','%Y-%m-%dT%H:%M:%S%z',
                  '%Y-%m-%dT%H:%M:%S.%f%z'
              ]:
                  try:
                      return datetime.fromisoformat(s.replace('Z','+00:00')) if 'T' in s else datetime.strptime(s, fmt)
                  except Exception:
                      pass
              m = re.search(r'((?:19|20)\d{2})[-/](\d{1,2})[-/](\d{1,2})', s)
              if m:
                  try:
                      return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
                  except Exception:
                      pass
              return None
          def context_score(text):
              t = norm(text)
              score = 0
              if 'video game' in t or 'videogame' in t:
                  score += 8
              if 'game store' in t:
                  score += 6
              if 'gaming' in t:
                  score += 2
              if 'game' in t:
                  score += 2
              if 'store' in t or 'shop' in t:
                  score += 2
              if 'website' in t:
                  score += 2
              if re.search(r'\bweb\b', t):
                  score += 1
              if 'page view' in t or 'pageview' in t:
                  score += 3
              return score
          def pageview_key_score(k):
              nk = norm_key(k)
              score = 0
              if 'pageview' in nk:
                  score += 10
              if nk in {'views','viewcount'}:
                  score += 3
              if 'visit' in nk or 'traffic' in nk:
                  score += 2
              if 'page' in nk:
                  score += 2
              if 'view' in nk:
                  score += 1
              return score
          def date_key_score(k):
              nk = norm_key(k)
              score = 0
              if nk in {'date','day'}:
                  score += 10
              if 'date' in nk:
                  score += 6
              if 'day' in nk:
                  score += 5
              if 'time' in nk or 'timestamp' in nk:
                  score += 3
              if 'created' in nk:
                  score += 2
              return score

          def fetch_arxiv_titles(target_date):
              week_start = target_date - timedelta(days=target_date.weekday())
              week_end = week_start + timedelta(days=6)
              res = {
                  'week_start': week_start.isoformat(),
                  'week_end': week_end.isoformat(),
                  'titles': [],
                  'entries_checked': []
              }
              queries = [
                  'au:"Wayne Polyzou"',
                  'au:"Wayne N. Polyzou"',
                  '"Wayne Polyzou"',
                  '"W. N. Polyzou"',
              ]
              seen_ids = set()
              for q in queries:
                  try:
                      url = 'https://export.arxiv.org/api/query?' + urllib.parse.urlencode({
                          'search_query': q,
                          'start': 0,
                          'max_results': 50,
                          'sortBy': 'submittedDate',
                          'sortOrder': 'descending',
                      })
                      with urllib.request.urlopen(url, timeout=60) as resp:
                          xml_data = resp.read()
                      root = ET.fromstring(xml_data)
                      ns = {'a': 'http://www.w3.org/2005/Atom'}
                      for entry in root.findall('a:entry', ns):
                          id_text = (entry.findtext('a:id', default='', namespaces=ns) or '').strip()
                          if not id_text or id_text in seen_ids:
                              continue
                          seen_ids.add(id_text)
                          title = re.sub(r'\s+', ' ', (entry.findtext('a:title', default='', namespaces=ns) or '')).strip()
                          published_raw = (entry.findtext('a:published', default='', namespaces=ns) or '').strip()
                          authors = [re.sub(r'\s+', ' ', (a.findtext('a:name', default='', namespaces=ns) or '')).strip() for a in entry.findall('a:author', ns)]
                          pd = datetime.fromisoformat(published_raw.replace('Z', '+00:00')).date() if published_raw else None
                          rec = {'id': id_text, 'title': title, 'published': published_raw, 'authors': authors}
                          if len(res['entries_checked']) < 50:
                              res['entries_checked'].append(rec)
                          auth_text = ' '.join(authors).lower()
                          if ('polyzou' in auth_text) and pd and week_start <= pd <= week_end:
                              res['titles'].append(title)
                  except Exception as e:
                      res.setdefault('errors', []).append(f'{q}: {e}')
              # Deduplicate preserving order
              dedup = []
              seen = set()
              for t in res['titles']:
                  if t not in seen:
                      seen.add(t)
                      dedup.append(t)
              res['titles'] = dedup
              return res

          try:
              time.sleep(20)
              client = None
              last = None
              for attempt in range(1, 11):
                  try:
                      client = MongoClient(os.environ['MONGO_URI'], serverSelectionTimeoutMS=20000, connectTimeoutMS=20000, socketTimeoutMS=20000)
                      result['ping'] = ser(client.admin.command('ping'))
                      result['connected_on_attempt'] = attempt
                      break
                  except Exception as e:
                      last = repr(e)
                      result.setdefault('connect_attempts', []).append({'attempt': attempt, 'error': repr(e)})
                      time.sleep(15)
              if client is None:
                  raise RuntimeError(f'connect failed: {last}')

              dbs = [d for d in client.list_database_names() if d not in ('admin','local','config')]
              result['databases'] = dbs

              collection_summaries = {}
              candidates = []

              for d in dbs:
                  db = client[d]
                  try:
                      colls = db.list_collection_names()
                  except Exception as e:
                      collection_summaries[d] = {'error': str(e)}
                      continue
                  collection_summaries[d] = {}
                  for c in colls:
                      coll = db[c]
                      top_keys = {}
                      sample_docs = []
                      scanned = 0
                      local_candidates = []
                      try:
                          cursor = coll.find({})
                      except Exception as e:
                          collection_summaries[d][c] = {'error': str(e)}
                          continue
                      for doc in cursor:
                          scanned += 1
                          if len(sample_docs) < 2:
                              sample_docs.append(ser(doc))
                          flat = flatten(doc)
                          for k in flat.keys():
                              top_keys[k] = top_keys.get(k, 0) + 1

                          # Build lightweight context text
                          text_parts = [d, c]
                          for k,v in list(flat.items())[:80]:
                              text_parts.append(k)
                              if isinstance(v, str) and len(v) <= 80:
                                  text_parts.append(v)
                          text_blob = ' | '.join(map(str, text_parts))
                          cscore = context_score(text_blob)

                          date_options = []
                          pv_options = []
                          for k,v in flat.items():
                              dk = date_key_score(k)
                              if dk:
                                  dt = parse_date(v)
                                  if dt:
                                      date_options.append((dk, k, dt))
                              pk = pageview_key_score(k)
                              num = numeric(v)
                              if pk and num is not None:
                                  pv_options.append((pk, k, num))
                          if date_options and pv_options:
                              date_options.sort(key=lambda x: (-x[0], x[1]))
                              pv_options.sort(key=lambda x: (-x[0], -x[2], x[1]))
                              best_date = date_options[0]
                              best_pv = pv_options[0]
                              cand = {
                                  'db': d,
                                  'collection': c,
                                  'date': best_date[2].date().isoformat(),
                                  'date_field': best_date[1],
                                  'page_views': best_pv[2],
                                  'page_view_field': best_pv[1],
                                  'context_score': cscore,
                                  'sample': ser(doc),
                              }
                              local_candidates.append(cand)
                          if scanned >= 5000:
                              break

                      # collection-level summary
                      key_list = sorted(top_keys.items(), key=lambda kv: (-kv[1], kv[0]))[:50]
                      summary_text = ' '.join([d, c] + [k for k,_ in key_list])
                      summary_score = context_score(summary_text)
                      collection_summaries[d][c] = {
                          'scanned': scanned,
                          'summary_score': summary_score,
                          'top_keys': key_list,
                          'sample_docs': sample_docs
                      }
                      # promote local candidates with collection summary
                      for cand in local_candidates:
                          cand['collection_summary_score'] = summary_score
                          cand['combined_score'] = cand['context_score'] + summary_score
                      if local_candidates:
                          # keep only strongest few per collection
                          local_candidates.sort(key=lambda x: (x['combined_score'], x['page_views']), reverse=True)
                          candidates.extend(local_candidates[:10])

              # Rank candidates
              def rank_tuple(x):
                  # prioritize plausible collection context strongly, then max page views
                  return (
                      1 if x.get('combined_score', 0) >= 6 else 0,
                      1 if x.get('combined_score', 0) >= 10 else 0,
                      x.get('combined_score', 0),
                      x.get('page_views', 0)
                  )
              candidates.sort(key=rank_tuple, reverse=True)

              result['candidate_count'] = len(candidates)
              result['top_candidates'] = candidates[:30]
              result['collection_summaries'] = collection_summaries

              chosen = None
              strong = [x for x in candidates if x.get('combined_score',0) >= 6]
              if strong:
                  chosen = max(strong, key=lambda x: x['page_views'])
              elif candidates:
                  chosen = max(candidates, key=lambda x: x['page_views'])

              result['chosen_traffic_row'] = chosen

              if chosen:
                  target_date = datetime.fromisoformat(chosen['date']).date()
                  arxiv = fetch_arxiv_titles(target_date)
                  result['arxiv_lookup'] = arxiv
                  titles = arxiv.get('titles') or []
                  result['answer'] = {
                      'peak_page_view_date': chosen['date'],
                      'page_views': chosen['page_views'],
                      'paper_title': titles[0] if titles else None,
                      'all_titles_that_week': titles
                  }

          except Exception as e:
              result['error'] = str(e)
              result['traceback'] = traceback.format_exc()

          os.makedirs('outputs', exist_ok=True)
          with open('outputs/video_game_store_pageviews_and_wayne_polyzou.json', 'w', encoding='utf-8') as f:
              json.dump(result, f, indent=2, default=ser)
          preview = {
              'connected_on_attempt': result.get('connected_on_attempt'),
              'candidate_count': result.get('candidate_count'),
              'chosen_traffic_row': result.get('chosen_traffic_row'),
              'answer': result.get('answer'),
              'error': result.get('error')
          }
          print(json.dumps(preview, indent=2, default=ser)[:20000])
          PY

      - name: Commit result
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com
          git add outputs/whitelist_result.json outputs/video_game_store_pageviews_and_wayne_polyzou.json
          git commit -m "Add video game store answer" || exit 0
          for i in 1 2 3 4 5; do
            git pull --rebase origin main && git push && exit 0 || true
            sleep 10
          done
          exit 0
